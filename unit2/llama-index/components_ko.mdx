# LlamaIndex의 컴포넌트란 무엇인가요?

유닛 1에서 소개한 우리의 도움이 되는 집사 에이전트 알프레드를 기억하시나요?
알프레드가 우리를 효과적으로 도울 수 있으려면, 우리의 요청을 이해하고 **작업 완료를 위해 관련 정보를 준비, 찾기, 사용할 수 있어야 합니다.**
여기서 LlamaIndex의 컴포넌트가 등장합니다.

LlamaIndex에는 많은 컴포넌트가 있지만, **특별히 `QueryEngine` 컴포넌트에 초점을 맞출 것입니다.**
왜냐고요? 에이전트를 위한 검색 증강 생성(RAG) 도구로 사용할 수 있기 때문입니다.

그렇다면 RAG란 무엇일까요? LLM은 일반 지식을 학습하기 위해 방대한 양의 데이터에 대해 훈련됩니다.
그러나 관련성 있고 최신 데이터에 대해서는 훈련되지 않았을 수 있습니다.
RAG는 데이터에서 관련 정보를 찾아 검색하고 이를 LLM에 제공함으로써 이 문제를 해결합니다.

![RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/rag.png)

이제 알프레드가 어떻게 작동하는지 생각해 보세요:

1. 알프레드에게 저녁 파티 계획을 도와달라고 요청합니다
2. 알프레드는 당신의 일정, 식이 선호도, 과거 성공적인 메뉴를 확인해야 합니다
3. `QueryEngine`은 알프레드가 이 정보를 찾고 저녁 파티를 계획하는 데 사용하도록 도와줍니다

이것이 `QueryEngine`을 **LlamaIndex에서 에이전트 RAG 워크플로우를 구축하기 위한 핵심 컴포넌트**로 만듭니다.
알프레드가 도움이 되기 위해 가정 정보를 검색해야 하는 것처럼, 모든 에이전트는 관련 데이터를 찾고 이해하는 방법이 필요합니다.
`QueryEngine`은 정확히 이러한 기능을 제공합니다.

이제 컴포넌트에 대해 더 깊이 들어가서 **컴포넌트를 결합하여 RAG 파이프라인을 만드는 방법**을 살펴보겠습니다.

## 컴포넌트를 사용하여 RAG 파이프라인 만들기

<Tip>
<a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/components.ipynb" target="_blank">이 노트북</a>에서 코드를 따라할 수 있으며, Google Colab을 사용하여 실행할 수 있습니다.
</Tip>

RAG 내에는 다섯 가지 주요 단계가 있으며, 이는 대부분의 더 큰 응용 프로그램을 구축할 때 일부가 됩니다. 이러한 단계는 다음과 같습니다:

1. **로딩**: 데이터가 위치한 곳(텍스트 파일, PDF, 다른 웹사이트, 데이터베이스 또는 API)에서 워크플로우로 가져오는 것을 의미합니다. LlamaHub는 선택할 수 있는 수백 개의 통합을 제공합니다.
2. **인덱싱**: 데이터 쿼리를 위한 데이터 구조를 만드는 것을 의미합니다. LLM의 경우, 이는 거의 항상 벡터 임베딩을 만드는 것을 의미합니다. 이는 데이터의 의미를 수치적으로 표현한 것입니다. 인덱싱은 또한 속성을 기반으로 문맥적으로 관련 있는 데이터를 정확하게 찾기 쉽게 하기 위한 다양한 다른 메타데이터 전략을 지칭할 수 있습니다.
3. **저장**: 데이터를 인덱싱한 후에는 다시 인덱싱할 필요가 없도록 인덱스와 기타 메타데이터를 저장하고 싶을 것입니다.
4. **쿼리**: 모든 인덱싱 전략에 대해 하위 쿼리, 다단계 쿼리 및 하이브리드 전략을 포함하여 LLM과 LlamaIndex 데이터 구조를 활용하여 쿼리하는 많은 방법이 있습니다.
5. **평가**: 모든 흐름에서 중요한 단계는 다른 전략에 비해 얼마나 효과적인지, 또는 변경할 때 확인하는 것입니다. 평가는 쿼리에 대한 응답이 얼마나 정확하고, 충실하며, 빠른지에 대한 객관적인 척도를 제공합니다.

다음으로, 컴포넌트를 사용하여 이러한 단계를 재현하는 방법을 살펴보겠습니다.

### 문서 로딩 및 임베딩

앞서 언급했듯이, LlamaIndex는 자신의 데이터 위에서 작동할 수 있지만, **데이터에 접근하기 전에 로드해야 합니다.**
LlamaIndex에 데이터를 로드하는 세 가지 주요 방법이 있습니다:

1. `SimpleDirectoryReader`: 로컬 디렉토리에서 다양한 파일 유형을 위한 내장 로더입니다.
2. `LlamaParse`: LlamaIndex의 공식 PDF 파싱 도구인 LlamaParse로, 관리형 API로 제공됩니다.
3. `LlamaHub`: 모든 소스에서 데이터를 가져오기 위한 수백 개의 데이터 로딩 라이브러리 레지스트리입니다.

<Tip>더 복잡한 데이터 소스를 위해 <a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/">LlamaHub</a> 로더와 <a href="https://github.com/run-llama/llama_cloud_services/blob/main/parse.md">LlamaParse</a> 파서에 익숙해지세요.</Tip>

**데이터를 로드하는 가장 간단한 방법은 `SimpleDirectoryReader`를 사용하는 것입니다.**
이 다목적 컴포넌트는 폴더에서 다양한 파일 유형을 로드하고 LlamaIndex가 작업할 수 있는 `Document` 객체로 변환할 수 있습니다.
`SimpleDirectoryReader`를 사용하여 폴더에서 데이터를 로드하는 방법을 살펴보겠습니다.

```python
from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(input_dir="path/to/directory")
documents = reader.load_data()
```

문서를 로드한 후, 이를 `Node` 객체라고 불리는 더 작은 조각으로 나눠야 합니다.
`Node`는 원본 문서에서 나온 텍스트 청크로, AI가 작업하기 더 쉬우면서도 원본 `Document` 객체에 대한 참조를 여전히 가지고 있습니다.

`IngestionPipeline`은 두 가지 주요 변환을 통해 이러한 노드를 만드는 데 도움을 줍니다.
1. `SentenceSplitter`는 자연스러운 문장 경계에서 나누어 문서를 관리 가능한 청크로 분해합니다.
2. `HuggingFaceInferenceAPIEmbedding`은 각 청크를 수치적 임베딩으로 변환합니다 - AI가 효율적으로 처리할 수 있는 방식으로 의미론적 의미를 포착하는 벡터 표현입니다.

이 과정은 검색과 분석에 더 유용한 방식으로 문서를 구성하는 데 도움이 됩니다.

```python
from llama_index.core import Document
from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline

# 변환이 있는 파이프라인 생성
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_overlap=0),
        HuggingFaceInferenceAPIEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ]
)

nodes = await pipeline.arun(documents=[Document.example()])
``` 

### 문서 저장 및 인덱싱

`Node` 객체를 생성한 후, 검색 가능하게 만들기 위해 인덱싱해야 하지만, 그 전에 데이터를 저장할 장소가 필요합니다.

수집 파이프라인을 사용하고 있으므로 벡터 저장소를 파이프라인에 직접 연결하여 채울 수 있습니다.
이 경우, 문서를 저장하기 위해 `Chroma`를 사용할 것입니다.

<details>
<summary>ChromaDB 설치</summary>
[LlamaHub 섹션](llama-hub)에서 소개한 대로, 다음 명령으로 ChromaDB 벡터 저장소를 설치할 수 있습니다:

```bash
pip install llama-index-vector-stores-chroma
```
</details>

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore

db = chromadb.PersistentClient(path="./alfred_chroma_db")
chroma_collection = db.get_or_create_collection("alfred")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        HuggingFaceInferenceAPIEmbedding(model_name="BAAI/bge-small-en-v1.5"),
    ],
    vector_store=vector_store,
)
```

<Tip>다양한 벡터 저장소에 대한 개요는 <a href="https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/">LlamaIndex 문서</a>에서 찾을 수 있습니다.</Tip>


여기서 벡터 임베딩이 등장합니다 - 쿼리와, 노드를 동일한 벡터 공간에 임베딩함으로써 관련된 일치 항목을 찾을 수 있습니다.
`VectorStoreIndex`는 일관성을 보장하기 위해 수집 중에 사용한 것과 동일한 임베딩 모델을 사용하여 이를 처리합니다.

벡터 저장소와 임베딩에서 이 인덱스를 생성하는 방법을 살펴보겠습니다:

```python
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding

embed_model = HuggingFaceInferenceAPIEmbedding(model_name="BAAI/bge-small-en-v1.5")
index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)
```

모든 정보는 `ChromaVectorStore` 객체와 전달된 디렉토리 경로 내에 자동으로 유지됩니다.

훌륭합니다! 이제 인덱스를 쉽게 저장하고 로드할 수 있으므로, 다양한 방법으로 쿼리하는 방법을 살펴보겠습니다.

### 프롬프트와 LLM을 사용하여 VectorStoreIndex 쿼리하기

인덱스를 쿼리하기 전에, 이를 쿼리 인터페이스로 변환해야 합니다. 가장 일반적인 변환 옵션은 다음과 같습니다:

- `as_retriever`: 기본 문서 검색을 위해, 유사성 점수가 있는 `NodeWithScore` 객체 목록을 반환합니다.
- `as_query_engine`: 단일 질문-답변 상호작용을 위해, 작성된 응답을 반환합니다.
- `as_chat_engine`: 여러 메시지에 걸쳐 메모리를 유지하는 대화형 상호작용을 위해, 채팅 기록과 인덱싱된 컨텍스트를 사용하여 작성된 응답을 반환합니다.

에이전트와 같은 상호작용에서 더 일반적이므로 쿼리 엔진에 초점을 맞출 것입니다.
또한 응답을 위해 사용할 LLM을 쿼리 엔진에 전달합니다.

```python
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
query_engine = index.as_query_engine(
    llm=llm,
    response_mode="tree_summarize",
)
query_engine.query("What is the meaning of life?")
# The meaning of life is 42
```

### 응답 처리

내부적으로, 쿼리 엔진은 질문에 답하기 위해 LLM만 사용하는 것이 아니라 응답을 처리하기 위한 전략으로 `ResponseSynthesizer`도 사용합니다.
다시 한번, 이는 완전히 사용자 정의 가능하지만 기본적으로 잘 작동하는 세 가지 주요 전략이 있습니다:

- `refine`: 각 검색된 텍스트 청크를 순차적으로 살펴보며 답변을 생성하고 수정합니다. 이는 노드/검색된 청크당 별도의 LLM 호출을 만듭니다.
- `compact` (기본값): 사전에 청크를 연결하여 정제하는 것과 유사하지만, LLM 호출이 적습니다.
- `tree_summarize`: 각 검색된 텍스트 청크를 살펴보고 답변의 트리 구조를 생성하여 상세한 답변을 만듭니다.

<Tip><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/#low-level-composition-api">저수준 구성 API</a>를 통해 쿼리 워크플로우를 세밀하게 제어하세요. 이 API를 사용하면 정확한 요구에 맞게 쿼리 프로세스의 모든 단계를 사용자 정의하고 미세 조정할 수 있으며, <a href="https://docs.llamaindex.ai/en/stable/module_guides/workflow/">워크플로우</a>와도 잘 연결됩니다.</Tip>

언어 모델이 항상 예측 가능한 방식으로 작동하지는 않으므로, 얻은 답변이 항상 정확하다고 확신할 수 없습니다. 이를 **응답 품질을 평가**하여 처리할 수 있습니다.

### 평가 및 관찰 가능성

LlamaIndex는 **응답 품질을 평가하기 위한 내장 도구를 제공합니다.**
이러한 평가자는 LLM을 활용하여 다양한 차원에서 응답을 분석합니다.
사용 가능한 세 가지 주요 평가자를 살펴보겠습니다:

- `FaithfulnessEvaluator`: 답변이 컨텍스트에 의해 지원되는지 확인하여 답변의 충실도를 평가합니다.
- `AnswerRelevancyEvaluator`: 답변이 질문과 관련 있는지 확인하여 답변의 관련성을 평가합니다.
- `CorrectnessEvaluator`: 답변이 올바른지 확인하여 답변의 정확성을 평가합니다.

```python
from llama_index.core.evaluation import FaithfulnessEvaluator

query_engine = # 이전 섹션에서 가져옴
llm = # 이전 섹션에서 가져옴

# 인덱스 쿼리
evaluator = FaithfulnessEvaluator(llm=llm)
response = query_engine.query(
    "What battles took place in New York City in the American Revolution?"
)
eval_result = evaluator.evaluate_response(response=response)
eval_result.passing
```

직접적인 평가 없이도, **관찰 가능성을 통해 시스템이 어떻게 작동하고 있는지에 대한 통찰을 얻을 수 있습니다.**
이는 특히 더 복잡한 워크플로우를 구축하고 각 컴포넌트가 어떻게 작동하는지 이해하고 싶을 때 유용합니다.

<details>
<summary>LlamaTrace 설치</summary>
[LlamaHub 섹션](llama-hub)에서 소개한 대로, 다음 명령으로 Arize Phoenix에서 LlamaTrace 콜백을 설치할 수 있습니다:

```bash
pip install -U llama-index-callbacks-arize-phoenix
```

또한, `PHOENIX_API_KEY` 환경 변수를 LlamaTrace API 키로 설정해야 합니다. 다음과 같이 얻을 수 있습니다:
- [LlamaTrace](https://llamatrace.com/login)에서 계정 생성
- 계정 설정에서 API 키 생성
- 아래 코드에서 API 키를 사용하여 추적 활성화

</details>

```python
import llama_index
import os

PHOENIX_API_KEY = "<PHOENIX_API_KEY>"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"api_key={PHOENIX_API_KEY}"
llama_index.core.set_global_handler(
    "arize_phoenix",
    endpoint="https://llamatrace.com/v1/traces"
)
```

<Tip>컴포넌트와 그 사용 방법에 대해 더 알고 싶으신가요? <a href="https://docs.llamaindex.ai/en/stable/module_guides/">컴포넌트 가이드</a> 또는 <a href="https://docs.llamaindex.ai/en/stable/understanding/rag/">RAG에 관한 가이드</a>와 함께 여정을 계속하세요.</Tip>

컴포넌트를 사용하여 `QueryEngine`을 만드는 방법을 살펴보았습니다. 이제, **`QueryEngine`을 에이전트를 위한 도구로 사용하는 방법**을 살펴보겠습니다! 